{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "qe-WiSu6c-1j"
   },
   "outputs": [],
   "source": [
    "# Problem to be solved\n",
    "# Build a naïve Bayes classifier for sentiment classification. We are defining\n",
    "# sentiment classification as two classes: positive and negative. Our data set consists of airline reviews. The\n",
    "# zip directory for the data contains training and test datasets, where each file contains one airline review\n",
    "# tweet. You will build the model using training data and evaluate with test data. Each of training data and\n",
    "# test data contains 4182 reviews. You will have to build the system from the scratch (e.g. numpy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "7Uai7lttc-1k"
   },
   "outputs": [],
   "source": [
    "# Give actual examples of program input and output, along with usage instructions.\n",
    "# Loading data for frequency with no stemming\n",
    "# Loading data for class 'positive' - Number of documents: 1181\n",
    "# Loading data for class 'negative' - Number of documents: 3000\n",
    "# Loading data for class 'positive' - Number of documents: 1182\n",
    "# Loading data for class 'negative' - Number of documents: 3000\n",
    "# Training Naive Bayes classifier with frequency mode...\n",
    "# Naive Bayes training complete.\n",
    "# Calculating performance metrics...\n",
    "# Metrics - Accuracy: 0.8867, Precision: 0.8800, Recall: 0.9750, F1: 0.9250\n",
    "# Completed frequency with no stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "B8ETXxeQc-1k"
   },
   "outputs": [],
   "source": [
    "#Describe the algorithm you have used to solve the problem, specified in a stepwise or point by point fashion.\n",
    "\n",
    "# load_and_preprocess_data\n",
    "    # Define the classes and prepare containers for data and vocabulary.\n",
    "    # For each class, load text files, preprocess the text, and store each document’s tokens with its class label.\n",
    "    # Add each unique token from the documents to the vocabulary set.\n",
    "    # Output the processed data and the vocabulary.\n",
    "# preprocess\n",
    "    # Use BeautifulSoup to remove any HTML tags and extract plain text from HTML content.\n",
    "    # Strip URLs and non-alphanumeric characters, replacing them with whitespace.\n",
    "    # Convert text to lowercase, split it into tokens, and apply stemming if enabled.\n",
    "    # Output the preprocessed list of tokens.\n",
    "# create_bow\n",
    "    # Initialize a bag of words (BoW) as a dictionary to count occurrences.\n",
    "    # For each word in each document, count occurrences; if binary, only count unique appearances per document.\n",
    "    # Return BoW dictionary with word counts.\n",
    "# calculate_tf\n",
    "    # Divide each word's count by the total number of words in the document.\n",
    "    # Return term frequency (TF) dictionary.\n",
    "# calculate_idf\n",
    "    # Count documents containing each word to calculate document frequency.\n",
    "    # For each word, compute IDF as log(total docs / (1 + document frequency)).\n",
    "    # Return IDF scores.\n",
    "# calculate_tfidf\n",
    "    # Compute term frequency for each word in the document.\n",
    "    # Multiply TF values by corresponding IDF scores to get TF-IDF values.\n",
    "    # Return TF-IDF scores.\n",
    "# train_naive_bayes\n",
    "    # Split documents by class and calculate class priors as the probability of each class based on document count.\n",
    "    # Calculate word frequencies per class and smooth probabilities (for non-TF-IDF modes).\n",
    "    # If using TF-IDF, compute likelihoods with TF-IDF values; otherwise, use word frequencies.\n",
    "    # Return priors and likelihoods for each class.\n",
    "# predict\n",
    "    # Initialize log probability scores with class priors for each class (scores[cls] = math.log(priors[cls])).\n",
    "    # For each word in the document, update the log score based on the word likelihood if present in the class.\n",
    "    # Return the class with the highest final score.\n",
    "# evaluate_naive_bayes\n",
    "    # For each test document, predict its class and store actual and predicted classes.\n",
    "    # Collect detailed prediction results and return them with actuals and predictions.\n",
    "# calculate_performance_metrics\n",
    "    # Build a confusion matrix from actual vs. predicted classes.\n",
    "    # Calculate accuracy, precision, recall, and F1 score for each class based on confusion matrix values.\n",
    "    # Return performance metrics and confusion matrix.\n",
    "\n",
    "#Set Paths and Configurations: Define training and testing data paths, representation modes (`frequency`, `binary`, `tfidf`), and stemming options.\n",
    "# Run Experiments: For each combination of representation mode and stemming option:\n",
    "    # Setup Logging\n",
    "    # Load and Preprocess Data\n",
    "    # Train Naive Bayes\n",
    "    # Evaluate Model\n",
    "    # Log Results\n",
    "    # Output Completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "oF2gicl1c-1l"
   },
   "outputs": [],
   "source": [
    "# Additional description: Please state whether the bonus credit questions are answered or not\n",
    "# Yes we have answered the bonus credit question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 5.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting regex>=2021.8.3\n",
      "  Downloading regex-2025.9.1-cp39-cp39-macosx_11_0_arm64.whl (286 kB)\n",
      "\u001b[K     |████████████████████████████████| 286 kB 34.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting joblib\n",
      "  Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "\u001b[K     |████████████████████████████████| 308 kB 58.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /Users/shalinimaknoor/Library/Python/3.9/lib/python/site-packages (from nltk) (4.67.1)\n",
      "Collecting click\n",
      "  Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "\u001b[K     |████████████████████████████████| 98 kB 11.3 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: regex, joblib, click, nltk\n",
      "Successfully installed click-8.1.8 joblib-1.5.2 nltk-3.9.1 regex-2025.9.1\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.2 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ATs21FFU7wL",
    "outputId": "019aff1f-e612-421c-b0f1-86a4ae8da2a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading data for frequency with stemming\n",
      "Loading data for class 'positive' - Number of documents: 0\n",
      "Loading data for class 'negative' - Number of documents: 0\n",
      "Loading data for class 'positive' - Number of documents: 0\n",
      "Loading data for class 'negative' - Number of documents: 0\n",
      "Training Naive Bayes classifier with frequency mode...\n",
      "Naive Bayes training complete.\n",
      "Calculating performance metrics...\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 174\u001b[0m\n\u001b[1;32m    171\u001b[0m actuals, predictions, prediction_details \u001b[38;5;241m=\u001b[39m evaluate_naive_bayes(test_data, priors, likelihoods)\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m# Calculate performance metrics (now using the updated function for single values)\u001b[39;00m\n\u001b[0;32m--> 174\u001b[0m confusion_matrix, accuracy, precision, recall, f1_score \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_performance_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactuals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMetrics - Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Precision: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprecision\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Recall: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrecall\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, F1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf1_score\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    177\u001b[0m log_file\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=Performance Metrics:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 132\u001b[0m, in \u001b[0;36mcalculate_performance_metrics\u001b[0;34m(actuals, predictions)\u001b[0m\n\u001b[1;32m    129\u001b[0m     j \u001b[38;5;241m=\u001b[39m class_index[predicted]\n\u001b[1;32m    130\u001b[0m     confusion_matrix[i][j] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 132\u001b[0m tp \u001b[38;5;241m=\u001b[39m \u001b[43mconfusion_matrix\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    133\u001b[0m fn \u001b[38;5;241m=\u001b[39m confusion_matrix[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    134\u001b[0m fp \u001b[38;5;241m=\u001b[39m confusion_matrix[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import glob\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.stem import SnowballStemmer, PorterStemmer # type: ignore\n",
    "\n",
    "# data loading from dataset folder and data distribution\n",
    "def load_and_preprocess_data(base_path, stemming_enabled=False):\n",
    "    classes = [\"positive\", \"negative\"]\n",
    "    data = []\n",
    "    vocabulary = set()\n",
    "\n",
    "    for cls in classes:\n",
    "        files = glob.glob(os.path.join(base_path, cls, '*.txt'))\n",
    "        print(f\"Loading data for class '{cls}' - Number of documents: {len(files)}\")\n",
    "        for file_path in files:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "                tokens = preprocess(text, stemming_enabled=stemming_enabled)\n",
    "                data.append({\"class\": cls, \"text\": tokens})\n",
    "                vocabulary.update(tokens)\n",
    "\n",
    "    return data, vocabulary\n",
    "\n",
    "def preprocess(text, stemming_enabled=False, stemmer_type=\"snowball\"):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    text = soup.get_text()\n",
    "\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    tokens = text.lower().split()\n",
    "    if stemming_enabled:\n",
    "        stemmer = SnowballStemmer('english') if stemmer_type == \"snowball\" else PorterStemmer()\n",
    "        tokens = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# bag of words creation for binary and frequency\n",
    "def create_bow(data, binary=False):\n",
    "    bow = defaultdict(int)\n",
    "    for document in data:\n",
    "        seen_words = set()\n",
    "        for word in document:\n",
    "            if binary:\n",
    "                if word not in seen_words:\n",
    "                    bow[word] += 1\n",
    "                    seen_words.add(word)\n",
    "            else:\n",
    "                bow[word] += 1\n",
    "    return bow\n",
    "\n",
    "# TF-IDF Calculation Functions\n",
    "def calculate_tf(word_counts, total_words_in_doc):\n",
    "    return {word: count / total_words_in_doc for word, count in word_counts.items()}\n",
    "\n",
    "def calculate_idf(train_data, vocabulary):\n",
    "    num_docs = len(train_data)\n",
    "    doc_freq = defaultdict(int)\n",
    "    for doc in train_data:\n",
    "        unique_words = set(doc['text'])\n",
    "        for word in unique_words:\n",
    "            doc_freq[word] += 1\n",
    "    return {word: math.log(num_docs / (1 + freq)) for word, freq in doc_freq.items()}\n",
    "\n",
    "def calculate_tfidf(word_counts, total_words_in_doc, idf_scores):\n",
    "    tf_scores = calculate_tf(word_counts, total_words_in_doc)\n",
    "    return {word: tf_scores[word] * idf_scores.get(word, 0) for word in tf_scores}\n",
    "\n",
    "# Training the Naive Bayes classifier with TF-IDF support\n",
    "def train_naive_bayes(data, vocabulary, representation=\"frequency\"):\n",
    "    class_docs = defaultdict(list)\n",
    "    for doc in data:\n",
    "        class_docs[doc[\"class\"]].append(doc[\"text\"])\n",
    "\n",
    "    total_docs = len(data)\n",
    "    priors = {cls: len(class_docs[cls]) / total_docs for cls in class_docs}\n",
    "    likelihoods = {}\n",
    "    idf_scores = calculate_idf(data, vocabulary) if representation == \"tfidf\" else None\n",
    "\n",
    "    for cls in class_docs:\n",
    "        word_counts = create_bow(class_docs[cls], binary=(representation == \"binary\"))\n",
    "        total_words_in_class = sum(word_counts.values())\n",
    "\n",
    "        if representation == \"tfidf\":\n",
    "            likelihoods[cls] = calculate_tfidf(word_counts, total_words_in_class, idf_scores)\n",
    "        else:\n",
    "            likelihoods[cls] = {\n",
    "                word: (word_counts.get(word, 0) + 1) / (total_words_in_class + len(vocabulary))\n",
    "                for word in vocabulary\n",
    "            }\n",
    "\n",
    "    return priors, likelihoods\n",
    "\n",
    "# Predicting the class output of a document based on priors and likelihoods\n",
    "def predict(document, priors, likelihoods):\n",
    "    scores = {cls: math.log(priors[cls]) for cls in priors}\n",
    "    for cls in scores:\n",
    "        for word in document:\n",
    "            if word in likelihoods[cls]:\n",
    "                scores[cls] += math.log(likelihoods[cls][word])\n",
    "    return max(scores, key=scores.get)\n",
    "\n",
    "# Naive Bayes evaluation\n",
    "def evaluate_naive_bayes(test_data, priors, likelihoods):\n",
    "    actuals, predictions = [], []\n",
    "    prediction_details = []\n",
    "\n",
    "    for doc_id, doc in enumerate(test_data):\n",
    "        actual = doc[\"class\"]\n",
    "        predicted = predict(doc[\"text\"], priors, likelihoods)\n",
    "        actuals.append(actual)\n",
    "        predictions.append(predicted)\n",
    "        prediction_details.append((doc_id, predicted, actual))\n",
    "\n",
    "    return actuals, predictions, prediction_details\n",
    "\n",
    "# Performance metrics calculation (Confusion Matrix, Accuracy, Precision, Recall, F1-score)\n",
    "def calculate_performance_metrics(actuals, predictions):\n",
    "    print(\"Calculating performance metrics...\")\n",
    "    classes = sorted(set(actuals))\n",
    "    class_index = {cls: i for i, cls in enumerate(classes)}\n",
    "    confusion_matrix = [[0] * len(classes) for _ in classes]\n",
    "\n",
    "    for actual, predicted in zip(actuals, predictions):\n",
    "        i = class_index[actual]\n",
    "        j = class_index[predicted]\n",
    "        confusion_matrix[i][j] += 1\n",
    "\n",
    "    tp = confusion_matrix[0][0]\n",
    "    fn = confusion_matrix[0][1]\n",
    "    fp = confusion_matrix[1][0]\n",
    "    tn = confusion_matrix[1][1]\n",
    "\n",
    "    # Metrics calculation\n",
    "    accuracy = (tp + tn) / float(sum(sum(row) for row in confusion_matrix))\n",
    "    precision = tp / float(tp + fp) if tp + fp != 0 else 0\n",
    "    recall = tp / float(tp + fn) if tp + fn != 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "\n",
    "    return confusion_matrix, accuracy, precision, recall, f1_score\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    path_to_training_data = \"/home/zoro/Workspace/GMU_Masters/Term 3/Assignments/AIT 526/Group Assignments/PA3/tweet/tweet/train\"\n",
    "    path_to_test_data = \"/home/zoro/Workspace/GMU_Masters/Term 3/Assignments/AIT 526/Group Assignments/PA3/tweet/tweet/test\"\n",
    "    representations = [\"frequency\", \"binary\", \"tfidf\"]\n",
    "    stem_options = [True, False]\n",
    "\n",
    "    # Run experiments for each configuration\n",
    "    for representation in representations:\n",
    "        for stemming_enabled in stem_options:\n",
    "            stem_status = \"stemmed\" if stemming_enabled else \"non-stemmed\"\n",
    "            config_name = f\"{representation}_{stem_status}\"\n",
    "            log_filename = f\"log_{config_name}.txt\"\n",
    "\n",
    "            with open(log_filename, \"w\") as log_file:\n",
    "                print(f\"\\nLoading data for {representation} with {'stemming' if stemming_enabled else 'no stemming'}\")\n",
    "\n",
    "                train_data, vocab = load_and_preprocess_data(path_to_training_data, stemming_enabled)\n",
    "                test_data, _ = load_and_preprocess_data(path_to_test_data, stemming_enabled)\n",
    "\n",
    "                # Training the Naive Bayes classifier\n",
    "                print(f\"Training Naive Bayes classifier with {representation} mode...\")\n",
    "                log_file.write(f\"Training Naive Bayes classifier with {representation} mode...\\n\")\n",
    "                priors, likelihoods = train_naive_bayes(train_data, vocab, representation=representation)\n",
    "                print(\"Naive Bayes training complete.\")\n",
    "                log_file.write(\"Naive Bayes training complete.\\n\")\n",
    "                actuals, predictions, prediction_details = evaluate_naive_bayes(test_data, priors, likelihoods)\n",
    "\n",
    "                # Calculate performance metrics (now using the updated function for single values)\n",
    "                confusion_matrix, accuracy, precision, recall, f1_score = calculate_performance_metrics(actuals, predictions)\n",
    "                print(f\"Metrics - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1_score:.4f}\")\n",
    "\n",
    "                log_file.write(\"\\n=Performance Metrics:\\n\")\n",
    "                log_file.write(\"Confusion Matrix:\\n\")\n",
    "                for row in confusion_matrix:\n",
    "                    log_file.write(f\"{row}\\n\")\n",
    "                log_file.write(f\"Accuracy: {accuracy:.4f}\\n\")\n",
    "                log_file.write(f\"Precision: {precision:.4f}\\n\")\n",
    "                log_file.write(f\"Recall: {recall:.4f}\\n\")\n",
    "                log_file.write(f\"F1 Score: {f1_score:.4f}\\n\\n\")\n",
    "                log_file.write(\"Predictions Log:\\nDoc ID, Predicted Class, Actual Class\\n\")\n",
    "                for doc_id, predicted, actual in prediction_details:\n",
    "                    log_file.write(f\"{doc_id + 1}, {predicted}, {actual}\\n\")\n",
    "                print(f\"Completed {representation} with {'stemming' if stemming_enabled else 'no stemming'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ps9nlNfeePGZ"
   },
   "outputs": [],
   "source": [
    "# References\n",
    "# https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "# https://stackoverflow.com/questions/328356/extracting-text-from-html-file-using-python/51209579\n",
    "# https://www.datacamp.com/tutorial/naive-bayes-scikit-learn"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
